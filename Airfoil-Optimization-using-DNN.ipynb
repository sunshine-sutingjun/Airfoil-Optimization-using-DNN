{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44f0698",
   "metadata": {},
   "source": [
    "## hp_activation.py\n",
    "\n",
    "This code is from the file `hp_activation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59530fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar  8 23:58:43 2021\n",
    "\n",
    "@author: lihen\n",
    "\"\"\"\n",
    "\n",
    "#import relevant packages and set some important parameters\n",
    "import time\n",
    "start_time = time.time()\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# Python 3.5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "result = pd.read_pickle('./naca4_clcd_turb_st_3para.pkl', compression=None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#set the random seeds required for initialization\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(1615400000)\n",
    "   tf.random.set_seed(1615400000)\n",
    "   np.random.seed(1615400000)\n",
    "   random.seed(1615400000)\n",
    "gg=1000\n",
    "\n",
    "\n",
    "#load data from naca4_clcd_turb_st_3para.pkl\n",
    "inp_reno=[]\n",
    "inp_aoa=[]\n",
    "inp_para=[]\n",
    "\n",
    "\n",
    "out_cm=[]\n",
    "out_cd=[]\n",
    "out_cl=[]\n",
    "\n",
    "\n",
    "out_cm.extend(result[0])   \n",
    "out_cd.extend(result[1])\n",
    "out_cl.extend(result[2])\n",
    "\n",
    "inp_reno.extend(result[3])\n",
    "inp_aoa.extend(result[4])\n",
    "inp_para.extend(result[5])\n",
    "\n",
    "out_cm=np.asarray(out_cm)/0.188171\n",
    "out_cd=np.asarray(out_cd)/0.2466741\n",
    "out_cl=np.asarray(out_cl)/1.44906\n",
    "\n",
    "\n",
    "out_cd=np.asarray(out_cd)\n",
    "out_cl=np.asarray(out_cl)\n",
    "\n",
    "inp_reno=np.asarray(inp_reno)\n",
    "inp_aoa=np.asarray(inp_aoa)\n",
    "inp_para=np.asarray(inp_para)/np.array([6,6,30])\n",
    "\n",
    "N= len(out_cm)\n",
    "print(N)\n",
    "I = np.arange(N)\n",
    "np.random.shuffle(I)\n",
    "n=N\n",
    "\n",
    "#normalize the numeral values such that the max value is 1\n",
    "inp_reno=inp_reno/100000.\n",
    "inp_aoa=inp_aoa/14.0\n",
    "\n",
    "my_inp=np.concatenate((inp_reno[:,None],inp_aoa[:,None],inp_para[:,:]),axis=1)\n",
    "my_out=np.concatenate((out_cd[:,None],out_cl[:,None]),axis=1)\n",
    "## Training sets\n",
    "xtr0 = my_inp[I][:n]\n",
    "ttr1 = my_out[I][:n]\n",
    "\n",
    "#list of activation functions to train model\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['elu',\n",
    "                                                     'exponential',\n",
    "                                                    \n",
    "                                                     'hard_sigmoid',\n",
    "                                                     'relu',\n",
    "                                                     'selu',\n",
    "                                                     'sigmoid',\n",
    "                                                     'softmax',\n",
    "                                                     'softplus',\n",
    "                                                     'softsign',\n",
    "                                                     'swish',\n",
    "                                                     'tanh']))\n",
    "METRIC_MSE = 'Mean Squared Error'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/activation').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_ACTIVATION],\n",
    "    metrics=[hp.Metric(METRIC_MSE, display_name='val_loss')],\n",
    "    )\n",
    "    \n",
    "epochs = list(range(0,gg))\n",
    "\n",
    "#this function sets the activation function of the model for training based on the list given.\n",
    "def train_test_model(hparams):\n",
    "    activation_name = hparams[HP_ACTIVATION]\n",
    "    if activation_name == \"elu\":\n",
    "        a = tf.keras.activations.elu\n",
    "    elif activation_name == \"exponential\":\n",
    "        a = tf.keras.activations.exponential\n",
    "    elif activation_name == \"gelu\":\n",
    "        a = tf.keras.activations.gelu\n",
    "    elif activation_name == \"hard_sigmoid\":\n",
    "        a = tf.keras.activations.hard_sigmoid\n",
    "    elif activation_name == \"relu\":\n",
    "        a = tf.keras.activations.relu\n",
    "    elif activation_name == \"selu\":\n",
    "        a = tf.keras.activations.selu\n",
    "    elif activation_name == \"sigmoid\":\n",
    "        a = tf.keras.activations.sigmoid\n",
    "    elif activation_name == \"softmax\":\n",
    "        a = tf.keras.activations.softmax\n",
    "    elif activation_name == \"softplus\":\n",
    "        a = tf.keras.activations.softplus \n",
    "    elif activation_name == \"softsign\":\n",
    "        a = tf.keras.activations.softsign\n",
    "    elif activation_name == \"swish\":\n",
    "        a = tf.keras.activations.swish   \n",
    "    elif activation_name == \"tanh\":\n",
    "        a = tf.keras.activations.tanh\n",
    "    else:\n",
    "        raise ValueError(\"unexpected optimizer name: %r\" % (activation_name))\n",
    "    xx = 90\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(5,)))\n",
    "    model.add(tf.keras.layers.Dense(xx, kernel_initializer='random_normal', activation=a))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=a))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=a))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=a))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=a))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=None))\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "  # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_name)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, mode='min',verbose=1 ,patience=100, min_lr=1.0e-8)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    history = model.fit([xtr0], [ttr1], validation_split=0.1,callbacks=[reduce_lr], epochs=gg)\n",
    "    mse = np.array(history.history['val_loss'])\n",
    "    return mse\n",
    "\n",
    "#records the loss function of this model \n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    mse1 = train_test_model(hparams)\n",
    "    for idx, epoch in enumerate(epochs):\n",
    "        mse = mse1[idx]\n",
    "        tf.summary.scalar(METRIC_MSE, mse, step=epoch+1)\n",
    "    \n",
    "session_num = 0\n",
    "#tensorboard --logdir='C:/Users/lihen/projects/tf-gpu-MNIST/logs/hparam_tuning5layer' is the path to called Tensorboard for comparing models\n",
    "\n",
    "#print details and keep logs upon script execution\n",
    "for activation in HP_ACTIVATION.domain.values:\n",
    "    hparams = {\n",
    "        HP_ACTIVATION: activation\n",
    "        }\n",
    "    run_name = \"{}\".format(activation)\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    run('logs/activation/' + run_name, hparams)\n",
    "    session_num += 1\n",
    "    reset_random_seeds()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b951528",
   "metadata": {},
   "source": [
    "## hp_batch.py\n",
    "\n",
    "This code is from the file `hp_batch.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar  9 00:04:43 2021\n",
    "\n",
    "@author: lihen\n",
    "\"\"\"\n",
    "\n",
    "#import relevant packages and set some important parameters\n",
    "import time\n",
    "start_time = time.time()\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# Python 3.5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "result = pd.read_pickle('./naca4_clcd_turb_st_3para.pkl', compression=None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(1615400000)\n",
    "   tf.random.set_seed(1615400000)\n",
    "   np.random.seed(1615400000)\n",
    "   random.seed(1615400000)\n",
    "gg=1000\n",
    "#load data from naca4_clcd_turb_st_3para.pkl\n",
    "inp_reno=[]\n",
    "inp_aoa=[]\n",
    "inp_para=[]\n",
    "\n",
    "\n",
    "out_cm=[]\n",
    "out_cd=[]\n",
    "out_cl=[]\n",
    "\n",
    "\n",
    "out_cm.extend(result[0])   \n",
    "out_cd.extend(result[1])\n",
    "out_cl.extend(result[2])\n",
    "\n",
    "inp_reno.extend(result[3])\n",
    "inp_aoa.extend(result[4])\n",
    "inp_para.extend(result[5])\n",
    "\n",
    "out_cm=np.asarray(out_cm)/0.188171\n",
    "out_cd=np.asarray(out_cd)/0.2466741\n",
    "out_cl=np.asarray(out_cl)/1.44906\n",
    "\n",
    "\n",
    "out_cd=np.asarray(out_cd)\n",
    "out_cl=np.asarray(out_cl)\n",
    "\n",
    "inp_reno=np.asarray(inp_reno)\n",
    "inp_aoa=np.asarray(inp_aoa)\n",
    "inp_para=np.asarray(inp_para)/np.array([6,6,30])\n",
    "\n",
    "N= len(out_cm)\n",
    "print(N)\n",
    "I = np.arange(N)\n",
    "np.random.shuffle(I)\n",
    "n=N\n",
    "\n",
    "#normalize the numeral values such that the max value is 1\n",
    "inp_reno=inp_reno/100000.\n",
    "inp_aoa=inp_aoa/14.0\n",
    "\n",
    "my_inp=np.concatenate((inp_reno[:,None],inp_aoa[:,None],inp_para[:,:]),axis=1)\n",
    "my_out=np.concatenate((out_cd[:,None],out_cl[:,None]),axis=1)\n",
    "## Training sets\n",
    "xtr0 = my_inp[I][:n]\n",
    "ttr1 = my_out[I][:n]\n",
    "\n",
    "\n",
    "#list of discrete batch sizes to train model\n",
    "HP_BATCH= hp.HParam('batch_size', hp.Discrete([16,32,64,128,256,512]))\n",
    "METRIC_MSE = 'Mean Squared Error'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/batch').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_BATCH],\n",
    "    metrics=[hp.Metric(METRIC_MSE, display_name='val_loss')],\n",
    "  )\n",
    "\n",
    "epochs = list(range(0,gg))\n",
    "\n",
    "#this function sets the batch size of the model for training based on the list given.\n",
    "def train_test_model(hparams):\n",
    "    xx = 90\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(5,)))\n",
    "    model.add(tf.keras.layers.Dense(xx, kernel_initializer='random_normal', activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=None))\n",
    "    model.summary()\n",
    "    \n",
    "    size = hparams[HP_BATCH] \n",
    "    if size == 16:\n",
    "        b = 16\n",
    "    elif size == 32:\n",
    "        b = 32\n",
    "    elif size == 64:\n",
    "        b= 64\n",
    "    elif size == 128:\n",
    "        b= 128\n",
    "    elif size == 256:\n",
    "        b= 256\n",
    "    elif size == 512:\n",
    "        b= 512\n",
    "    else:\n",
    "        raise ValueError(\"unexpected size: %r\" % (size))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "  # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_name)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, mode='min',verbose=1 ,patience=100, min_lr=1.0e-8)\n",
    "    history = model.fit([xtr0], [ttr1], validation_split=0.1,callbacks=[reduce_lr], batch_size=b, epochs=gg)\n",
    "    mse = np.array(history.history['val_loss'])\n",
    "    return mse\n",
    "\n",
    "#records the loss function of this model \n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    mse1 = train_test_model(hparams)\n",
    "    for idx, epoch in enumerate(epochs):\n",
    "        mse = mse1[idx]\n",
    "        tf.summary.scalar(METRIC_MSE, mse, step=epoch+1)\n",
    "    \n",
    "session_num = 0\n",
    "#tensorboard --logdir='C:/Users/lihen/projects/tf-gpu-MNIST/logs/hparam_tuning5layer' is the path to called Tensorboard for comparing models\n",
    "\n",
    "#print details and keep logs upon script execution\n",
    "for batch in HP_BATCH.domain.values:\n",
    "    hparams = {\n",
    "        HP_BATCH: batch\n",
    "        }\n",
    "    run_name = \"%d\" % (batch)\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    run('logs/batch/' + run_name, hparams)\n",
    "    session_num += 1\n",
    "    reset_random_seeds()\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b8e7f",
   "metadata": {},
   "source": [
    "## hp_learningrate.py\n",
    "\n",
    "This code is from the file `hp_learningrate.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb  2 14:19:39 2021\n",
    "\n",
    "@author: lihen\n",
    "\"\"\"\n",
    "\n",
    "#import relevant packages and set some important parameters\n",
    "import time\n",
    "start_time = time.time()\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# Python 3.5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "result = pd.read_pickle('./naca4_clcd_turb_st_3para.pkl', compression=None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#load data from naca4_clcd_turb_st_3para.pkl\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(1615400000)\n",
    "   tf.random.set_seed(1615400000)\n",
    "   np.random.seed(1615400000)\n",
    "   random.seed(1615400000)\n",
    "gg=1000\n",
    "#load data\n",
    "inp_reno=[]\n",
    "inp_aoa=[]\n",
    "inp_para=[]\n",
    "\n",
    "\n",
    "out_cm=[]\n",
    "out_cd=[]\n",
    "out_cl=[]\n",
    "\n",
    "\n",
    "out_cm.extend(result[0])   \n",
    "out_cd.extend(result[1])\n",
    "out_cl.extend(result[2])\n",
    "\n",
    "inp_reno.extend(result[3])\n",
    "inp_aoa.extend(result[4])\n",
    "inp_para.extend(result[5])\n",
    "\n",
    "out_cm=np.asarray(out_cm)/0.188171\n",
    "out_cd=np.asarray(out_cd)/0.2466741\n",
    "out_cl=np.asarray(out_cl)/1.44906\n",
    "\n",
    "\n",
    "out_cd=np.asarray(out_cd)\n",
    "out_cl=np.asarray(out_cl)\n",
    "\n",
    "inp_reno=np.asarray(inp_reno)\n",
    "inp_aoa=np.asarray(inp_aoa)\n",
    "inp_para=np.asarray(inp_para)/np.array([6,6,30])\n",
    "\n",
    "N= len(out_cm)\n",
    "print(N)\n",
    "I = np.arange(N)\n",
    "np.random.shuffle(I)\n",
    "n=N\n",
    "\n",
    "#normalize the numeral values such that the max value is 1\n",
    "inp_reno=inp_reno/100000.\n",
    "inp_aoa=inp_aoa/14.0\n",
    "\n",
    "my_inp=np.concatenate((inp_reno[:,None],inp_aoa[:,None],inp_para[:,:]),axis=1)\n",
    "my_out=np.concatenate((out_cd[:,None],out_cl[:,None]),axis=1)\n",
    "## Training sets\n",
    "xtr0 = my_inp[I][:n]\n",
    "ttr1 = my_out[I][:n]\n",
    "\n",
    "#list of learning rates to train model\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['nadam']))\n",
    "HP_L_RATE= hp.HParam('learning_rate', hp.Discrete([0.00001, \n",
    "                                                   0.000025, \n",
    "                                                   0.00005, \n",
    "                                                   0.000075, \n",
    "                                                   0.0001,\n",
    "                                                   0.00025,\n",
    "                                                   0.0005,\n",
    "                                                   0.00075,\n",
    "                                                   0.001,\n",
    "                                                   0.0025,\n",
    "                                                   0.005,\n",
    "                                                   0.0075,\n",
    "                                                   0.01]))\n",
    "METRIC_MSE = 'Mean Squared Error'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/learning rate').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_L_RATE],\n",
    "    metrics=[hp.Metric(METRIC_MSE, display_name='val_loss')],\n",
    "  )\n",
    "\n",
    "epochs = list(range(0,gg))\n",
    "\n",
    "#this function sets the learning rate of the model for training based on the list given. The base activation function is relu\n",
    "def train_test_model(hparams):\n",
    "    xx = 90\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(5,)))\n",
    "    model.add(tf.keras.layers.Dense(xx, kernel_initializer='random_normal', activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=None))\n",
    "    model.summary()\n",
    "    optimizer_name = hparams[HP_OPTIMIZER]\n",
    "    lr = hparams[HP_L_RATE]\n",
    "    if optimizer_name == \"nadam\":\n",
    "        o = tf.keras.optimizers.Nadam(learning_rate=lr)\n",
    "    else:\n",
    "        raise ValueError(\"unexpected optimizer name: %r\" % (optimizer_name,))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=o,\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "  # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_name)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, mode='min',verbose=1 ,patience=100, min_lr=1.0e-8)\n",
    "    history = model.fit([xtr0], [ttr1], validation_split=0.1,callbacks=[reduce_lr], epochs=gg)\n",
    "    mse = np.array(history.history['val_loss'])\n",
    "    return mse\n",
    "\n",
    "#records the loss function of this model \n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    mse1 = train_test_model(hparams)\n",
    "    for idx, epoch in enumerate(epochs):\n",
    "        mse = mse1[idx]\n",
    "        tf.summary.scalar(METRIC_MSE, mse, step=epoch+1)\n",
    "    \n",
    "\n",
    "#tensorboard --logdir='C:/Users/lihen/projects/tf-gpu-MNIST/logs/hparam_tuning5layer'\n",
    "for optimizer in HP_OPTIMIZER.domain.values:\n",
    "    for learning_rate in HP_L_RATE.domain.values:\n",
    "        hparams = {\n",
    "            HP_OPTIMIZER: optimizer,\n",
    "            HP_L_RATE: learning_rate\n",
    "            }\n",
    "        run_name = \"{}\".format(learning_rate)\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        reset_random_seeds()\n",
    "        run('logs/learning rate/' + run_name, hparams)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ea16a",
   "metadata": {},
   "source": [
    "## hp_neurons.py\n",
    "\n",
    "This code is from the file `hp_neurons.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8162ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar  9 00:01:05 2021\n",
    "\n",
    "@author: lihen\n",
    "\"\"\"\n",
    "\n",
    "#import relevant packages and set some important parameters\n",
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# Python 3.5\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "result = pd.read_pickle(r'C:/Users/lihen/projects/tf-gpu-MNIST/naca4_clcd_turb_st_3para.pkl', compression=None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "layers = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "#set the random seeds required for initialization\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(1615400000)\n",
    "   tf.random.set_seed(1615400000)\n",
    "   np.random.seed(1615400000)\n",
    "   random.seed(1615400000)\n",
    "   os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "gg=1000\n",
    "\n",
    "#load data from naca4_clcd_turb_st_3para.pkl\n",
    "inp_reno=[]\n",
    "inp_aoa=[]\n",
    "inp_para=[]\n",
    "\n",
    "\n",
    "out_cm=[]\n",
    "out_cd=[]\n",
    "out_cl=[]\n",
    "\n",
    "\n",
    "out_cm.extend(result[0])   \n",
    "out_cd.extend(result[1])\n",
    "out_cl.extend(result[2])\n",
    "\n",
    "inp_reno.extend(result[3])\n",
    "inp_aoa.extend(result[4])\n",
    "inp_para.extend(result[5])\n",
    "\n",
    "out_cm=np.asarray(out_cm)/0.188171\n",
    "out_cd=np.asarray(out_cd)/0.2466741\n",
    "out_cl=np.asarray(out_cl)/1.44906\n",
    "\n",
    "\n",
    "out_cd=np.asarray(out_cd)\n",
    "out_cl=np.asarray(out_cl)\n",
    "\n",
    "inp_reno=np.asarray(inp_reno)\n",
    "inp_aoa=np.asarray(inp_aoa)\n",
    "inp_para=np.asarray(inp_para)/np.array([6,6,30])\n",
    "\n",
    "N= len(out_cm)\n",
    "print(N)\n",
    "I = np.arange(N)\n",
    "np.random.shuffle(I)\n",
    "n=N\n",
    "\n",
    "#normalize the numeral values such that the max value is 1\n",
    "inp_reno=inp_reno/100000.\n",
    "inp_aoa=inp_aoa/14.0\n",
    "\n",
    "my_inp=np.concatenate((inp_reno[:,None],inp_aoa[:,None],inp_para[:,:]),axis=1)\n",
    "my_out=np.concatenate((out_cd[:,None],out_cl[:,None]),axis=1)\n",
    "## Training sets\n",
    "xtr0 = my_inp[I][:n]\n",
    "ttr1 = my_out[I][:n]\n",
    "\n",
    "#Hyperparameters set from 1 to 10 layers with neurons in each layer in multiples of 10, up to 100\n",
    "HP_NEURONS = hp.HParam('neurons', hp.Discrete(range(10,101,10)))\n",
    "HP_LAYERS = hp.HParam('layer', hp.Discrete([1,2,3,4,5,6,7,8,9,10]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "HP_L_RATE= hp.HParam('learning_rate', hp.Discrete([2.5e-4]))\n",
    "METRIC_MSE = 'Mean Squared Error'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/1 hidden layer').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYERS, HP_NEURONS, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_MSE, display_name='val_loss')],\n",
    "  )\n",
    "\n",
    "epochs = list(range(0,gg))\n",
    "\n",
    "#this function builds the layers and neurons of the model for training based on the list given.\n",
    "def train_test_model(hparams, layer,rundir):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(5,)))\n",
    "    model.add(tf.keras.layers.Dense(hparams[HP_NEURONS], kernel_initializer='random_normal', activation=tf.keras.activations.relu))\n",
    "    if layer > 1: \n",
    "        for i in range(layer-1):\n",
    "             model.add(tf.keras.layers.Dense(hparams[HP_NEURONS], activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=None))\n",
    "    model.summary()\n",
    "    optimizer_name = hparams[HP_OPTIMIZER]\n",
    "    learning_rate = hparams[HP_L_RATE]\n",
    "    if optimizer_name == \"adam\":\n",
    "        o = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        o = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"unexpected optimizer name: %r\" % (optimizer_name,))\n",
    "    model.compile(\n",
    "        optimizer=o,\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "    # logdir=rundir\n",
    "    # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, profile_batch = 100000000)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, mode='min',verbose=0 ,patience=100, min_lr=1.0e-8)\n",
    "    history = model.fit([xtr0], [ttr1], validation_split=0.1,callbacks=[reduce_lr], epochs=gg,shuffle=False)\n",
    "    mse = np.array(history.history['val_loss'])\n",
    "    return mse\n",
    "\n",
    "#records the loss function of this model \n",
    "def run(run_dir, hparams, layer):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        mse1 = train_test_model(hparams, layer, run_dir)\n",
    "        for idx, epoch in enumerate(epochs):\n",
    "            mse = mse1[idx]\n",
    "            tf.summary.scalar(METRIC_MSE, mse, step=epoch+1)\n",
    "    \n",
    "#tensorboard --logdir='C:/Users/lihen/projects/tf-gpu-MNIST/logs/hparam_tuning5layer'\n",
    "for layer in HP_LAYERS.domain.values:\n",
    "    for neurons in HP_NEURONS.domain.values:\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            for learning_rate in HP_L_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_LAYERS: layer,\n",
    "                    HP_NEURONS: neurons,\n",
    "                    HP_OPTIMIZER: optimizer,\n",
    "                    HP_L_RATE: learning_rate\n",
    "                    }\n",
    "                run_name = \"{}\".format(neurons) + \" Neurons\"\n",
    "                print('--- Starting trial: %s' % run_name)\n",
    "                print(\"{} layer \".format(layer))\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                reset_random_seeds()\n",
    "                run('logs/{} hidden layer '.format(layer) + run_name, hparams, layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e232df8",
   "metadata": {},
   "source": [
    "## hp_optimizer.py\n",
    "\n",
    "This code is from the file `hp_optimizer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb  2 14:19:39 2021\n",
    "\n",
    "@author: lihen\n",
    "\"\"\"\n",
    "\n",
    "#import relevant packages and set some important parameters\n",
    "import time\n",
    "start_time = time.time()\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# Python 3.5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "result = pd.read_pickle('./naca4_clcd_turb_st_3para.pkl', compression=None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#set the random seeds required for initialization\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(1615400000)\n",
    "   tf.random.set_seed(1615400000)\n",
    "   np.random.seed(1615400000)\n",
    "   random.seed(1615400000)\n",
    "gg=1000\n",
    "\n",
    "#load data from naca4_clcd_turb_st_3para.pkl\n",
    "inp_reno=[]\n",
    "inp_aoa=[]\n",
    "inp_para=[]\n",
    "\n",
    "\n",
    "out_cm=[]\n",
    "out_cd=[]\n",
    "out_cl=[]\n",
    "\n",
    "\n",
    "out_cm.extend(result[0])   \n",
    "out_cd.extend(result[1])\n",
    "out_cl.extend(result[2])\n",
    "\n",
    "inp_reno.extend(result[3])\n",
    "inp_aoa.extend(result[4])\n",
    "inp_para.extend(result[5])\n",
    "\n",
    "out_cm=np.asarray(out_cm)/0.188171\n",
    "out_cd=np.asarray(out_cd)/0.2466741\n",
    "out_cl=np.asarray(out_cl)/1.44906\n",
    "\n",
    "\n",
    "out_cd=np.asarray(out_cd)\n",
    "out_cl=np.asarray(out_cl)\n",
    "\n",
    "inp_reno=np.asarray(inp_reno)\n",
    "inp_aoa=np.asarray(inp_aoa)\n",
    "inp_para=np.asarray(inp_para)/np.array([6,6,30])\n",
    "\n",
    "N= len(out_cm)\n",
    "print(N)\n",
    "I = np.arange(N)\n",
    "np.random.shuffle(I)\n",
    "n=N\n",
    "\n",
    "#normalize\n",
    "inp_reno=inp_reno/100000.\n",
    "inp_aoa=inp_aoa/14.0\n",
    "\n",
    "my_inp=np.concatenate((inp_reno[:,None],inp_aoa[:,None],inp_para[:,:]),axis=1)\n",
    "my_out=np.concatenate((out_cd[:,None],out_cl[:,None]),axis=1)\n",
    "## Training sets\n",
    "xtr0 = my_inp[I][:n]\n",
    "ttr1 = my_out[I][:n]\n",
    "\n",
    "#list of optimizers to train model\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['ftrl',\n",
    "    'adam', \n",
    "    'adadelta',\n",
    "    'adagrad', \n",
    "    'adamax',\n",
    "    'sgd',\n",
    "    'nadam',\n",
    "    'rmsprop'\n",
    "    ]))\n",
    "# HP_L_RATE= hp.HParam('learning_rate', hp.Discrete([2.5e-4]))\n",
    "METRIC_MSE = 'Mean Squared Error'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/optimizer/').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_MSE, display_name='val_loss')],\n",
    "  )\n",
    "\n",
    "epochs = list(range(0,gg))\n",
    "\n",
    "#this function sets the optimizer of the model for training based on the list given.\n",
    "def train_test_model(hparams):\n",
    "    xx = 90\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(5,)))\n",
    "    model.add(tf.keras.layers.Dense(xx, kernel_initializer='random_normal', activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=None))\n",
    "    model.summary()\n",
    "    optimizer_name = hparams[HP_OPTIMIZER]\n",
    "    if optimizer_name == \"adam\":\n",
    "        o = tf.keras.optimizers.Adam()\n",
    "    elif optimizer_name == \"adadelta\":\n",
    "        o = tf.keras.optimizers.Adadelta()\n",
    "    elif optimizer_name == \"adagrad\":\n",
    "        o = tf.keras.optimizers.Adagrad()\n",
    "    elif optimizer_name == \"adamax\":\n",
    "        o = tf.keras.optimizers.Adamax()\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        o = tf.keras.optimizers.SGD()\n",
    "    elif optimizer_name == \"nadam\":\n",
    "        o = tf.keras.optimizers.Nadam()\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        o = tf.keras.optimizers.RMSprop()\n",
    "    elif optimizer_name == \"ftrl\":\n",
    "        o = tf.keras.optimizers.Ftrl()\n",
    "    else:\n",
    "        raise ValueError(\"unexpected optimizer name: %r\" % (optimizer_name))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=o,\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "  # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_name)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, mode='min',verbose=1 ,patience=100, min_lr=1.0e-8)\n",
    "    history = model.fit([xtr0], [ttr1], validation_split=0.1,callbacks=[reduce_lr], epochs=gg, shuffle=False)\n",
    "    mse = np.array(history.history['val_loss'])\n",
    "    return mse\n",
    "\n",
    "#records the loss function of this model \n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    mse1 = train_test_model(hparams)\n",
    "    for idx, epoch in enumerate(epochs):\n",
    "        mse = mse1[idx]\n",
    "        tf.summary.scalar(METRIC_MSE, mse, step=epoch+1)\n",
    "    \n",
    "#tensorboard --logdir='C:/Users/lihen/projects/tf-gpu-MNIST/logs/hparam_tuning5layer'\n",
    "\n",
    "#print details and keep logs upon script execution\n",
    "for optimizer in HP_OPTIMIZER.domain.values:\n",
    "    hparams = {\n",
    "        HP_OPTIMIZER: optimizer\n",
    "        }\n",
    "    run_name = \"{}\".format(optimizer)\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    reset_random_seeds()\n",
    "    run('logs/optimizer/' + run_name, hparams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf5b1f",
   "metadata": {},
   "source": [
    "## main.py\n",
    "\n",
    "This code is from the file `main.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Feb  3 23:18:38 2021\n",
    "\n",
    "@author: lihen\n",
    "\"\"\"\n",
    "\n",
    "#import relevant packages and set some important parameters\n",
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# Python 3.5\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "result = pd.read_pickle(r'./naca4_clcd_turb_st_3para.pkl', compression=None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#set the random seeds required for initialization\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(1615400000)\n",
    "tf.random.set_seed(1615400000)\n",
    "np.random.seed(1615400000)\n",
    "random.seed(1615400000)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    \n",
    "#load data from naca4_clcd_turb_st_3para.pkl    \n",
    "#load data\n",
    "inp_reno=[]\n",
    "inp_aoa=[]\n",
    "inp_para=[]\n",
    "\n",
    "out_cm=[]\n",
    "out_cd=[]\n",
    "out_cl=[]\n",
    "\n",
    "out_cm.extend(result[0])   \n",
    "out_cd.extend(result[1])\n",
    "out_cl.extend(result[2])\n",
    "\n",
    "inp_reno.extend(result[3])\n",
    "inp_aoa.extend(result[4])\n",
    "inp_para.extend(result[5])\n",
    "\n",
    "out_cm=np.asarray(out_cm)/0.188171\n",
    "out_cd=np.asarray(out_cd)/0.2466741\n",
    "out_cl=np.asarray(out_cl)/1.44906\n",
    "\n",
    "\n",
    "out_cd=np.asarray(out_cd)\n",
    "out_cl=np.asarray(out_cl)\n",
    "\n",
    "inp_reno=np.asarray(inp_reno)\n",
    "inp_aoa=np.asarray(inp_aoa)\n",
    "inp_para=np.asarray(inp_para)/np.array([6,6,30])\n",
    "\n",
    "\n",
    "# ---------ML PART:-----------#\n",
    "#shuffle data\n",
    "N= len(out_cm)\n",
    "print(N)\n",
    "I = np.arange(N)\n",
    "np.random.shuffle(I)\n",
    "n=N\n",
    "\n",
    "#normalize the numeral values such that the max value is 1\n",
    "inp_reno=inp_reno/100000.\n",
    "inp_aoa=inp_aoa/14.0\n",
    "\n",
    "my_inp=np.concatenate((inp_reno[:,None],inp_aoa[:,None],inp_para[:,:]),axis=1)\n",
    "my_out=np.concatenate((out_cd[:,None],out_cl[:,None]),axis=1)\n",
    "\n",
    "## Training sets\n",
    "xtr0= my_inp[I][:n]\n",
    "ttr1 = my_out[I][:n]\n",
    "\n",
    "# Multilayer Perceptron\n",
    "# create model\n",
    "xx= 90\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(5,)))\n",
    "model.add(tf.keras.layers.Dense(xx, kernel_initializer='random_normal', activation=tf.keras.activations.swish))\n",
    "model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.swish))\n",
    "model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.swish))\n",
    "model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.swish))\n",
    "model.add(tf.keras.layers.Dense(xx, activation=tf.keras.activations.swish))\n",
    "model.add(tf.keras.layers.Dense(2, activation=None))\n",
    "\n",
    "#callbacks\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, mode='min',verbose=1 ,patience=100, min_lr=1.0e-8)\n",
    "\n",
    "#prevents overtraining when the ANN has converged at a local minimum\n",
    "e_stop = EarlyStopping(monitor='loss', min_delta=1.0e-8, patience=200, verbose=0, mode='auto')\n",
    "\n",
    "filepath=\"./model_optimized_notreduced/model_sf_{epoch:02d}_{loss:.8f}_{val_loss:.8f}.hdf5\"\n",
    "\n",
    "chkpt= ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\\\n",
    "                                save_best_only=False, save_weights_only=False, mode='auto', period=100)\n",
    "\n",
    "# Compile model\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer= opt, loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "hist = model.fit([xtr0], [ttr1], validation_split=0.1, batch_size=32, callbacks=[e_stop,chkpt],verbose=1,shuffle=False, epochs=10000)\n",
    "\n",
    "#save model\n",
    "model.save('./model_optimized_notreduced/final_sf.hdf5') \n",
    "\n",
    "print(\"\\n\") \n",
    "print(\"loss = %f to %f\"%(np.asarray(hist.history[\"loss\"][:1]),np.asarray(hist.history[\"loss\"][-1:])))\n",
    "print(\"\\n\")\n",
    "print(\"val_loss = %f to %f\"%(np.asarray(hist.history[\"val_loss\"][:1]),np.asarray(hist.history[\"val_loss\"][-1:])))\n",
    "print(\"\\n\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "data1=[hist.history]\n",
    "with open('./model_optimized_notreduced/hist.pkl', 'wb') as outfile:\n",
    "    pickle.dump(data1, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "model.summary()\n",
    "#%%\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
